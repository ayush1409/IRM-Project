{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRM Project (Twitter Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project members\n",
    "   ###### 1. Anuj Agrawal                    1509113020\n",
    "   ###### 2. Ayush Singh                    1509113031\n",
    "   ###### 3. Aakash patel                    1509113002\n",
    "   ###### 4. Abhilash Pandey             1509113003\n",
    "   ###### 5. Gautam Genda                 1509113041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connection through twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consumer_key = 'Vq4bB2uFtk8HPnTqeZiLeazKD'\n",
    "consumer_sectret = 'X82CzwP36l1HR2U5g2sMnsoDayq2AEF0LGUWCheD9bcnm4tfjO'\n",
    "\n",
    "access_token = '912685818661134336-ol8QdSYGhjF40fpSwFikOdS69P7akXL'\n",
    "access_token_secret = 'Th2c1FUSULuivBHlsjCmkrXI69bEr3FCTcTMpR9PTCJhq'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_sectret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch tweets by entering query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "public_tweets = api.search(q='indian elections', count=100)\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "for tweet in public_tweets:\n",
    "    tweet_list.append(tweet.text)\n",
    "    #print(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mark sentiments of each tweet using [textblob](http://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiments = [] \n",
    "for tweet in public_tweets:\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    if 0 < analysis.sentiment.polarity:\n",
    "        sentiments.append(1)\n",
    "    else:\n",
    "        sentiments.append(0)\n",
    "        \n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataset from the fetched tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  sentiment\n",
      "0   RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "1   RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "2   RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "3   RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "4   RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "5   RT @airnewsalerts: Record 20 Indian Americans ...          0\n",
      "6   RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "7   RT @airnewsalerts: Record 20 Indian Americans ...          0\n",
      "8   RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "9   RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "10  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "11  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "12  Top Fiji elections job for New Zealand woman -...          1\n",
      "13  @ANI The satement of Election wow now they wan...          1\n",
      "14  Top Fiji elections job for New Zealand woman -...          1\n",
      "15  @GoingOffensive @ShantiseAshanTi Any such move...          0\n",
      "16  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "17  RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "18  RT @airnewsalerts: Record 20 Indian Americans ...          0\n",
      "19  RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "20  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "21  RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "22  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "23  RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "24  RT @airnewsalerts: Record 20 Indian Americans ...          0\n",
      "25  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "26  RT @TVMohandasPai: The global leftist and urba...          0\n",
      "27  RT @prashantktm: The @htTweets Monday Intervie...          1\n",
      "28  The @htTweets Monday Interview- Jairam Ramesh ...          1\n",
      "29  RT @airnewsalerts: Record 20 Indian Americans ...          0\n",
      "..                                                ...        ...\n",
      "70  RT @TVMohandasPai: The global leftist and urba...          0\n",
      "71  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "72  RT @ShaktiShekhar: May prove to be a game-chan...          0\n",
      "73  @Joydas No party has ever got near to 50% in I...          1\n",
      "74  RT @ShaktiShekhar: May prove to be a game-chan...          0\n",
      "75  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "76  May prove to be a game-changer for 2019 Lok Sa...          0\n",
      "77  @republic Congress Party will be permanently r...          0\n",
      "78  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "79  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "80  @BeingKaNTRi @BleedbIue111 Karnataka elections...          1\n",
      "81  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "82  RT @TshumanMC: Mama, even those that pretend t...          1\n",
      "83  @BleedbIue111 Neeku Modi chesevanni alaage ani...          0\n",
      "84  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "85  RT @ssudjmc: We welcome eminent guests from PR...          1\n",
      "86  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "87  RT @NewIndianXpress: With six months still to ...          0\n",
      "88  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "89  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "90  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "91  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "92  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "93  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "94  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "95  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "96  RT @antidotdot: @eyafjallajokull @haloefekti @...          0\n",
      "97  RT @antidotdot: @eyafjallajokull @haloefekti @...          0\n",
      "98  RT @vineetjaintimes: My prediction-DEEP fake n...          0\n",
      "99  @eyafjallajokull @haloefekti @Ian56789 It is h...          0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_list = list(zip(tweet_list, sentiments))\n",
    "#print(combined_list)\n",
    "\n",
    "cols = ['tweet', 'sentiment']\n",
    "\n",
    "data = pd.DataFrame.from_records(combined_list, columns=cols)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       0.200000\n",
       "std        0.402015\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        1.000000\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X = data['tweet']\n",
    "y = data['sentiment']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into Train-Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75,)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Term Matrix using word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 191)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 38)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 185)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 136)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 142)\t1\n",
      "  (1, 153)\t1\n",
      "  (1, 61)\t1\n",
      "  (1, 182)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 87)\t1\n",
      "  (1, 105)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 126)\t1\n",
      "  (1, 173)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 67)\t1\n",
      "  (1, 142)\t1\n",
      "  (2, 159)\t1\n",
      "  :\t:\n",
      "  (72, 146)\t1\n",
      "  (72, 81)\t1\n",
      "  (72, 63)\t1\n",
      "  (72, 1)\t1\n",
      "  (72, 38)\t1\n",
      "  (72, 142)\t1\n",
      "  (73, 91)\t1\n",
      "  (73, 2)\t1\n",
      "  (73, 181)\t1\n",
      "  (73, 186)\t1\n",
      "  (73, 103)\t1\n",
      "  (73, 75)\t1\n",
      "  (73, 47)\t1\n",
      "  (73, 63)\t2\n",
      "  (73, 38)\t1\n",
      "  (74, 38)\t1\n",
      "  (74, 27)\t1\n",
      "  (74, 185)\t1\n",
      "  (74, 28)\t1\n",
      "  (74, 6)\t1\n",
      "  (74, 67)\t1\n",
      "  (74, 0)\t1\n",
      "  (74, 136)\t1\n",
      "  (74, 3)\t1\n",
      "  (74, 142)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(type(X_train_counts))\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '2019', '9iyk6uy0ej', 'airnewsalerts', 'alaage', 'allegations', 'americans', 'amp', 'ani', 'anipistaayi', 'antidotdot', 'anubhavpatnaik7', 'apr', 'attend', 'attract', 'bc', 'bet', 'bjp', 'bleedbiue111', 'breaking']\n",
      "['vdkmwvnnok', 'vg8gya2qzp', 'vineetjaintimes', 'voi', 'voters', 'votes', 'want', 'watershed', 'welcome', 'wellwishers', 'woman', 'worldwide', 'worse', 'wow', 'year', 'zealand', 'अम', 'रत', 'लड', 'सद']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[:20])\n",
    "print(vect.get_feature_names()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document term matrix using Tf-idf(Term Frequency- Inverse document frequency )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 191)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 142)\t0.14540645614132222\n",
      "  (0, 3)\t0.378558325033633\n",
      "  (0, 136)\t0.3658297503432734\n",
      "  (0, 0)\t0.3543153539413911\n",
      "  (0, 67)\t0.16900165236900064\n",
      "  (0, 6)\t0.3658297503432734\n",
      "  (0, 28)\t0.3658297503432734\n",
      "  (0, 185)\t0.3658297503432734\n",
      "  (0, 27)\t0.3543153539413911\n",
      "  (0, 38)\t0.143473459338603\n",
      "  (1, 142)\t0.19200850357098406\n",
      "  (1, 67)\t0.22316584306859918\n",
      "  (1, 38)\t0.18945599088793724\n",
      "  (1, 173)\t0.283224797727932\n",
      "  (1, 126)\t0.283224797727932\n",
      "  (1, 32)\t0.283224797727932\n",
      "  (1, 42)\t0.27435981498809664\n",
      "  (1, 105)\t0.283224797727932\n",
      "  (1, 87)\t0.283224797727932\n",
      "  (1, 25)\t0.283224797727932\n",
      "  (1, 7)\t0.283224797727932\n",
      "  (1, 182)\t0.283224797727932\n",
      "  (1, 61)\t0.283224797727932\n",
      "  (1, 153)\t0.283224797727932\n",
      "  (2, 142)\t0.10176245751312442\n",
      "  :\t:\n",
      "  (72, 146)\t0.33268915905862345\n",
      "  (72, 49)\t0.3163893498701205\n",
      "  (72, 151)\t0.37835770392188095\n",
      "  (72, 130)\t0.3526385132218122\n",
      "  (72, 23)\t0.3526385132218122\n",
      "  (72, 36)\t0.3526385132218122\n",
      "  (73, 38)\t0.09705387013491863\n",
      "  (73, 63)\t0.4286659580388574\n",
      "  (73, 47)\t0.322350551743775\n",
      "  (73, 75)\t0.322350551743775\n",
      "  (73, 103)\t0.322350551743775\n",
      "  (73, 186)\t0.322350551743775\n",
      "  (73, 181)\t0.322350551743775\n",
      "  (73, 2)\t0.3789964720567211\n",
      "  (73, 91)\t0.3789964720567211\n",
      "  (74, 142)\t0.14540645614132222\n",
      "  (74, 3)\t0.378558325033633\n",
      "  (74, 136)\t0.3658297503432734\n",
      "  (74, 0)\t0.3543153539413911\n",
      "  (74, 67)\t0.16900165236900064\n",
      "  (74, 6)\t0.3658297503432734\n",
      "  (74, 28)\t0.3658297503432734\n",
      "  (74, 185)\t0.3658297503432734\n",
      "  (74, 27)\t0.3543153539413911\n",
      "  (74, 38)\t0.143473459338603\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "print(type(X_train_tfidf))\n",
    "print(X_train_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 191)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 76)\t0.6623334127481719\n",
      "  (0, 38)\t0.1858609981430413\n",
      "  (0, 18)\t0.7257893218604363\n",
      "  (1, 182)\t0.283224797727932\n",
      "  (1, 173)\t0.283224797727932\n",
      "  (1, 153)\t0.283224797727932\n",
      "  (1, 142)\t0.19200850357098406\n",
      "  (1, 126)\t0.283224797727932\n",
      "  (1, 105)\t0.283224797727932\n",
      "  (1, 87)\t0.283224797727932\n",
      "  (1, 67)\t0.22316584306859918\n",
      "  (1, 61)\t0.283224797727932\n",
      "  (1, 42)\t0.27435981498809664\n",
      "  (1, 38)\t0.18945599088793724\n",
      "  (1, 32)\t0.283224797727932\n",
      "  (1, 25)\t0.283224797727932\n",
      "  (1, 7)\t0.283224797727932\n",
      "  (2, 182)\t0.283224797727932\n",
      "  (2, 173)\t0.283224797727932\n",
      "  (2, 153)\t0.283224797727932\n",
      "  (2, 142)\t0.19200850357098406\n",
      "  (2, 126)\t0.283224797727932\n",
      "  (2, 105)\t0.283224797727932\n",
      "  (2, 87)\t0.283224797727932\n",
      "  (2, 67)\t0.22316584306859918\n",
      "  :\t:\n",
      "  (23, 142)\t0.09154935472954602\n",
      "  (23, 121)\t0.28305291699891677\n",
      "  (23, 117)\t0.28305291699891677\n",
      "  (23, 107)\t0.30002588631391647\n",
      "  (23, 98)\t0.30002588631391647\n",
      "  (23, 90)\t0.28305291699891677\n",
      "  (23, 38)\t0.09033232066737737\n",
      "  (23, 34)\t0.28305291699891677\n",
      "  (23, 30)\t0.28305291699891677\n",
      "  (23, 16)\t0.30002588631391647\n",
      "  (23, 15)\t0.28305291699891677\n",
      "  (24, 178)\t0.27489176696082174\n",
      "  (24, 160)\t0.27489176696082174\n",
      "  (24, 159)\t0.2861816427200589\n",
      "  (24, 148)\t0.27489176696082174\n",
      "  (24, 142)\t0.10176245751312442\n",
      "  (24, 135)\t0.27489176696082174\n",
      "  (24, 125)\t0.2861816427200589\n",
      "  (24, 93)\t0.27489176696082174\n",
      "  (24, 83)\t0.27489176696082174\n",
      "  (24, 71)\t0.27489176696082174\n",
      "  (24, 69)\t0.27489176696082174\n",
      "  (24, 64)\t0.27489176696082174\n",
      "  (24, 55)\t0.27489176696082174\n",
      "  (24, 20)\t0.26493339078447553\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "print(X_test_tfidf.shape)\n",
    "print(type(X_test_tfidf))\n",
    "print(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.84\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy : \",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the model in two steps\n",
    "### A. Running the Agorithm using a pipeline that includes the following steps\n",
    "   ##### 1. creating Document-term matrix using word frequency of each word in document\n",
    "   ##### 2. creating Document matrix using Tf-idf term weighs\n",
    "   ##### 3. Fitting classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Optimising the algorithm by tunning the Model Parameters using **\"GridSearchCV\"** \n",
    "##### Tunning of Parameters includes-\n",
    "###### 1. we can choose whether to use uni-gram or bi-gram words for vocabulary\n",
    "###### 2. we can choose whether to Enable inverse-document-frequency reweighting or not (Desable by default).\n",
    "###### 3. adjusting the smoothening parameter (alpha  default_value = 1) in naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range' : [(1,1), (1,2)],\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'clf__alpha' : (1e-2, 1e-3)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (0.01, 0.001)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Model using optimal Tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 420)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 58)\t0.22941573387056174\n",
      "  (0, 409)\t0.22941573387056174\n",
      "  (0, 60)\t0.22941573387056174\n",
      "  (0, 15)\t0.22941573387056174\n",
      "  (0, 167)\t0.22941573387056174\n",
      "  (0, 2)\t0.22941573387056174\n",
      "  (0, 301)\t0.22941573387056174\n",
      "  (0, 9)\t0.22941573387056174\n",
      "  (0, 313)\t0.22941573387056174\n",
      "  (0, 82)\t0.22941573387056174\n",
      "  (0, 56)\t0.22941573387056174\n",
      "  (0, 408)\t0.22941573387056174\n",
      "  (0, 59)\t0.22941573387056174\n",
      "  (0, 14)\t0.22941573387056174\n",
      "  (0, 166)\t0.22941573387056174\n",
      "  (0, 0)\t0.22941573387056174\n",
      "  (0, 300)\t0.22941573387056174\n",
      "  (0, 8)\t0.22941573387056174\n",
      "  (0, 312)\t0.22941573387056174\n",
      "  (1, 141)\t0.19245008972987526\n",
      "  (1, 403)\t0.19245008972987526\n",
      "  (1, 17)\t0.19245008972987526\n",
      "  (1, 83)\t0.19245008972987526\n",
      "  (1, 168)\t0.19245008972987526\n",
      "  (1, 53)\t0.19245008972987526\n",
      "  :\t:\n",
      "  (73, 410)\t0.2182178902359924\n",
      "  (73, 237)\t0.2182178902359924\n",
      "  (73, 183)\t0.2182178902359924\n",
      "  (73, 112)\t0.2182178902359924\n",
      "  (73, 144)\t0.4364357804719848\n",
      "  (73, 82)\t0.2182178902359924\n",
      "  (74, 58)\t0.22941573387056174\n",
      "  (74, 409)\t0.22941573387056174\n",
      "  (74, 60)\t0.22941573387056174\n",
      "  (74, 15)\t0.22941573387056174\n",
      "  (74, 167)\t0.22941573387056174\n",
      "  (74, 2)\t0.22941573387056174\n",
      "  (74, 301)\t0.22941573387056174\n",
      "  (74, 9)\t0.22941573387056174\n",
      "  (74, 313)\t0.22941573387056174\n",
      "  (74, 82)\t0.22941573387056174\n",
      "  (74, 56)\t0.22941573387056174\n",
      "  (74, 408)\t0.22941573387056174\n",
      "  (74, 59)\t0.22941573387056174\n",
      "  (74, 14)\t0.22941573387056174\n",
      "  (74, 166)\t0.22941573387056174\n",
      "  (74, 0)\t0.22941573387056174\n",
      "  (74, 300)\t0.22941573387056174\n",
      "  (74, 8)\t0.22941573387056174\n",
      "  (74, 312)\t0.22941573387056174\n"
     ]
    }
   ],
   "source": [
    "vect1 = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "X_train_counts1 = vect1.fit_transform(X_train, y_train)\n",
    "\n",
    "tfidf_transformer1 = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
    "\n",
    "print(X_train_tfidf1.shape)\n",
    "print(type(X_train_tfidf1))\n",
    "print(X_train_tfidf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha=0.01)\n",
    "clf1.fit(X_train_tfidf1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 420)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 37)\t0.5\n",
      "  (0, 82)\t0.5\n",
      "  (0, 185)\t0.5\n",
      "  (0, 186)\t0.5\n",
      "  (1, 16)\t0.19245008972987526\n",
      "  (1, 17)\t0.19245008972987526\n",
      "  (1, 52)\t0.19245008972987526\n",
      "  (1, 53)\t0.19245008972987526\n",
      "  (1, 68)\t0.19245008972987526\n",
      "  (1, 69)\t0.19245008972987526\n",
      "  (1, 82)\t0.19245008972987526\n",
      "  (1, 83)\t0.19245008972987526\n",
      "  (1, 100)\t0.19245008972987526\n",
      "  (1, 102)\t0.19245008972987526\n",
      "  (1, 140)\t0.19245008972987526\n",
      "  (1, 141)\t0.19245008972987526\n",
      "  (1, 166)\t0.19245008972987526\n",
      "  (1, 168)\t0.19245008972987526\n",
      "  (1, 207)\t0.19245008972987526\n",
      "  (1, 208)\t0.19245008972987526\n",
      "  (1, 241)\t0.19245008972987526\n",
      "  (1, 242)\t0.19245008972987526\n",
      "  (1, 280)\t0.19245008972987526\n",
      "  (1, 281)\t0.19245008972987526\n",
      "  (1, 312)\t0.19245008972987526\n",
      "  :\t:\n",
      "  (24, 128)\t0.19245008972987526\n",
      "  (24, 129)\t0.19245008972987526\n",
      "  (24, 159)\t0.19245008972987526\n",
      "  (24, 160)\t0.19245008972987526\n",
      "  (24, 175)\t0.19245008972987526\n",
      "  (24, 176)\t0.19245008972987526\n",
      "  (24, 178)\t0.19245008972987526\n",
      "  (24, 179)\t0.19245008972987526\n",
      "  (24, 198)\t0.19245008972987526\n",
      "  (24, 199)\t0.19245008972987526\n",
      "  (24, 218)\t0.19245008972987526\n",
      "  (24, 219)\t0.19245008972987526\n",
      "  (24, 278)\t0.19245008972987526\n",
      "  (24, 279)\t0.19245008972987526\n",
      "  (24, 298)\t0.19245008972987526\n",
      "  (24, 299)\t0.19245008972987526\n",
      "  (24, 312)\t0.19245008972987526\n",
      "  (24, 319)\t0.19245008972987526\n",
      "  (24, 335)\t0.19245008972987526\n",
      "  (24, 336)\t0.19245008972987526\n",
      "  (24, 358)\t0.19245008972987526\n",
      "  (24, 359)\t0.19245008972987526\n",
      "  (24, 360)\t0.19245008972987526\n",
      "  (24, 393)\t0.19245008972987526\n",
      "  (24, 395)\t0.19245008972987526\n"
     ]
    }
   ],
   "source": [
    "X_test_counts1 = vect1.transform(X_test)\n",
    "X_test_tfidf1 = tfidf_transformer1.transform(X_test_counts1)\n",
    "\n",
    "print(X_test_tfidf1.shape)\n",
    "print(type(X_test_tfidf1))\n",
    "print(X_test_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.84\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = clf1.predict(X_test_tfidf1)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are not getting an improved accuracy after optimal tuning of parameters may be we should need to collect more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the result Obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18,  0],\n",
       "       [ 4,  3]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80    @BeingKaNTRi @BleedbIue111 Karnataka elections...\n",
      "82    RT @TshumanMC: Mama, even those that pretend t...\n",
      "56    (1) Watched two really great movies made in In...\n",
      "73    @Joydas No party has ever got near to 50% in I...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# All false negative\n",
    "print(X_test[y_test > y_pred1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@BeingKaNTRi @BleedbIue111 Karnataka elections kaadu.. General 😜'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @TshumanMC: Mama, even those that pretend to be genuine before elections are here. Once voted into office they reveal their true colours…'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(1) Watched two really great movies made in India called #Bhoothnath. Both great family movies, funny touching stor… https://t.co/d9QEGfjbDW'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Joydas No party has ever got near to 50% in Indian general elections, not even Rajiv Gandhi in 1985.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[73]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
