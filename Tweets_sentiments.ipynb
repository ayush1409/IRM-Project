{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IRM Project (Twitter Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Project members\n",
    "   ###### 1. Anuj Agrawal                    1509113020\n",
    "   ###### 2. Ayush Singh                    1509113031\n",
    "   ###### 3. Aakash patel                    1509113002\n",
    "   ###### 4. Abhilash Pandey             1509113003\n",
    "   ###### 5. Gautam Genda                 1509113041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Import all the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### connection through twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/python3_practice/lib/python3.5/site-packages/ipykernel_launcher.py:2: ResourceWarning: unclosed file <_io.BufferedReader name='secret_twitter_credentials.pkl'>\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "Twitter = pickle.load(open('secret_twitter_credentials.pkl','rb'))\n",
    "\n",
    "auth = tweepy.OAuthHandler(Twitter['consumer_key'], Twitter['consumer_secret'])\n",
    "auth.set_access_token(Twitter['access_token'], Twitter['access_token_secret'])\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fetch tweets by entering query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "public_tweets = api.search(q='indian election', count=100)\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "for tweet in public_tweets:\n",
    "    tweet_list.append(tweet.text)\n",
    "    #print(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### mark sentiments of each tweet using [textblob](http://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sentiments = [] \n",
    "for tweet in public_tweets:\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    if 0 < analysis.sentiment.polarity:\n",
    "        sentiments.append(1)\n",
    "    else:\n",
    "        sentiments.append(0)\n",
    "        \n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### creating dataset from the fetched tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  sentiment\n",
      "0   RT @MarioPuzo901: Wake up Siddhramiah ..Ur Pap...          0\n",
      "1   Wake up Siddhramiah ..Ur Pappu Vish..Vishrayya...          0\n",
      "2   Siddaramaiah-BJP face off over north Indian 'i...          0\n",
      "3   @TimesNow @siddaramaiah if PM and CM Yogi come...          0\n",
      "4   @TimesNow Sale the chutiye south Indian dirty ...          0\n",
      "5   @TimesNow Seeing certain defeat in karnataka e...          1\n",
      "6   @brainy_indian @dhume @divyaspandana This was ...          0\n",
      "7   @siddaramaiah @BJP4Karnataka @BSYBJP Mr. Pappu...          0\n",
      "8   @BDUTT @KapilSibal @arunjaitley @MVenkaiahNaid...          0\n",
      "9   RT @DVSBJP: Are you putting your words in our ...          0\n",
      "10  @FlanJerry @dhume @divyaspandana If this was s...          0\n",
      "11  RT @intercepted: .@RalphNader on Democrats, th...          0\n",
      "12  Kapil Sibal in 2010 when they were in power.\\n...          0\n",
      "13  RT @AP: BREAKING: Republican Debbie Lesko wins...          1\n",
      "14  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "15  RT @giniromet: Rahul Gandhi don't trust follow...          0\n",
      "16  RT @CTRavi_BJP: To those who don't respect and...          0\n",
      "17  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "18  @sardanarohit Rohitji now after declaring supr...          0\n",
      "19  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "20  @rammadhavbjp Many congratulations you being s...          1\n",
      "21  #AsaramBapu @narendramodi \\nBJP government is ...          0\n",
      "22  RT @shaitaankhopdi: @siddaramaiah @BJP4Karnata...          1\n",
      "23  RT @_Janhwi: This is not an ordinry election b...          0\n",
      "24  #africa #tech \\n\\nThe Indian general election ...          1\n",
      "25  RT @NEWS9TWEETS: #KarnatakaKurukshetra: High C...          1\n",
      "26  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "27  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "28  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "29  @Being_Saffron @shilpitewari Ji kannadigas are...          1\n",
      "..                                                ...        ...\n",
      "70  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "71  RT @ravishankar_iam: @siddaramaiah @BJP4Karnat...          0\n",
      "72  RT @Akshaysinghel: Think Twice before going to...          1\n",
      "73  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "74  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "75  RT @maryashakil: Why does Mohd Ibrahim a silk ...          0\n",
      "76  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "77  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "78  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "79  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "80  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "81  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "82  @Shehla_Rashid Which congress you mean?\\n1. In...          0\n",
      "83  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "84  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "85  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "86  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "87  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "88  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "89  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "90  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "91  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "92  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "93  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "94  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "95  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "96  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "97  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "98  RT @ravishankar_iam: @siddaramaiah @BJP4Karnat...          0\n",
      "99  RT @DVSBJP: Are you putting your words in our ...          0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_list = list(zip(tweet_list, sentiments))\n",
    "#print(combined_list)\n",
    "\n",
    "cols = ['tweet', 'sentiment']\n",
    "\n",
    "data = pd.DataFrame.from_records(combined_list, columns=cols)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.00000\n",
       "mean       0.15000\n",
       "std        0.35887\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.00000\n",
       "75%        0.00000\n",
       "max        1.00000\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X = data['tweet']\n",
    "y = data['sentiment']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Split the dataset into Train-Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75,)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create Document Term Matrix using word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 309)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 109)\t1\n",
      "  (0, 120)\t1\n",
      "  (0, 274)\t1\n",
      "  (0, 86)\t1\n",
      "  (0, 211)\t1\n",
      "  (0, 128)\t1\n",
      "  (0, 239)\t1\n",
      "  (0, 234)\t1\n",
      "  (1, 80)\t1\n",
      "  (1, 124)\t1\n",
      "  (1, 137)\t1\n",
      "  (1, 219)\t1\n",
      "  (1, 186)\t1\n",
      "  (1, 123)\t1\n",
      "  (1, 197)\t1\n",
      "  (1, 182)\t1\n",
      "  (1, 210)\t1\n",
      "  (1, 185)\t1\n",
      "  (1, 303)\t1\n",
      "  (1, 218)\t1\n",
      "  (1, 82)\t1\n",
      "  (1, 128)\t1\n",
      "  (1, 234)\t1\n",
      "  (2, 80)\t1\n",
      "  (2, 124)\t1\n",
      "  :\t:\n",
      "  (72, 293)\t1\n",
      "  (72, 127)\t1\n",
      "  (72, 234)\t1\n",
      "  (73, 0)\t1\n",
      "  (73, 230)\t1\n",
      "  (73, 281)\t1\n",
      "  (73, 227)\t1\n",
      "  (73, 208)\t1\n",
      "  (73, 73)\t1\n",
      "  (73, 194)\t1\n",
      "  (73, 212)\t1\n",
      "  (73, 2)\t1\n",
      "  (73, 252)\t1\n",
      "  (73, 147)\t1\n",
      "  (73, 120)\t1\n",
      "  (74, 222)\t1\n",
      "  (74, 176)\t1\n",
      "  (74, 37)\t1\n",
      "  (74, 134)\t1\n",
      "  (74, 36)\t1\n",
      "  (74, 29)\t1\n",
      "  (74, 253)\t1\n",
      "  (74, 197)\t1\n",
      "  (74, 86)\t1\n",
      "  (74, 128)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "print(type(X_train_counts))\n",
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0g3jry0ygj', '0zkbek1rbp', '2010', '2016', '_janhwi', 'abt', 'abvpjnu', 'africa', 'akshaysinghel', 'aljazeera_world', 'amp', 'anxious', 'ap', 'arizona', 'army', 'arunjaitley', 'asarambapu', 'awami', 'away', 'ayodhya']\n",
      "['vish', 'vishrayya', 'vishwarayya', 'voices', 'vote', 'vs', 'vsjxhev4na', 'waha', 'wake', 'wants', 'wat', 'win', 'wins', 'wit', 'words', 'worst', 'year', 'yogi', 'youth', 'zooaltrgnc']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[:20])\n",
    "print(vect.get_feature_names()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create Document term matrix using Tf-idf(Term Frequency- Inverse document frequency )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 309)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 234)\t0.14716998869348072\n",
      "  (0, 239)\t0.5086743893851138\n",
      "  (0, 128)\t0.13561350346136428\n",
      "  (0, 211)\t0.46420088330235926\n",
      "  (0, 86)\t0.208044907246352\n",
      "  (0, 274)\t0.40817089198748824\n",
      "  (0, 120)\t0.24566085784306554\n",
      "  (0, 109)\t0.46420088330235926\n",
      "  (1, 234)\t0.19709709259503685\n",
      "  (1, 128)\t0.18162009446458496\n",
      "  (1, 82)\t0.27862369706954065\n",
      "  (1, 218)\t0.27862369706954065\n",
      "  (1, 303)\t0.27862369706954065\n",
      "  (1, 185)\t0.27862369706954065\n",
      "  (1, 210)\t0.2739599523888023\n",
      "  (1, 182)\t0.2739599523888023\n",
      "  (1, 197)\t0.2174915580551883\n",
      "  (1, 123)\t0.24118114236732835\n",
      "  (1, 186)\t0.2739599523888023\n",
      "  (1, 219)\t0.25665814049778024\n",
      "  (1, 137)\t0.25665814049778024\n",
      "  (1, 124)\t0.27862369706954065\n",
      "  (1, 80)\t0.27862369706954065\n",
      "  (2, 234)\t0.19709709259503685\n",
      "  (2, 128)\t0.18162009446458496\n",
      "  :\t:\n",
      "  (72, 155)\t0.28429026243298994\n",
      "  (72, 292)\t0.28429026243298994\n",
      "  (72, 25)\t0.28429026243298994\n",
      "  (73, 120)\t0.1440932660850486\n",
      "  (73, 147)\t0.29836480578905467\n",
      "  (73, 252)\t0.29836480578905467\n",
      "  (73, 2)\t0.29836480578905467\n",
      "  (73, 212)\t0.29836480578905467\n",
      "  (73, 194)\t0.29836480578905467\n",
      "  (73, 73)\t0.29836480578905467\n",
      "  (73, 208)\t0.29836480578905467\n",
      "  (73, 227)\t0.29836480578905467\n",
      "  (73, 281)\t0.29836480578905467\n",
      "  (73, 230)\t0.29836480578905467\n",
      "  (73, 0)\t0.29836480578905467\n",
      "  (74, 128)\t0.11170919518085361\n",
      "  (74, 86)\t0.17137326709199316\n",
      "  (74, 197)\t0.133772680719052\n",
      "  (74, 253)\t0.22565389922397114\n",
      "  (74, 29)\t0.30582279718998095\n",
      "  (74, 36)\t0.30582279718998095\n",
      "  (74, 134)\t0.41901141993217517\n",
      "  (74, 37)\t0.41901141993217517\n",
      "  (74, 176)\t0.41901141993217517\n",
      "  (74, 222)\t0.41901141993217517\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "print(type(X_train_tfidf))\n",
    "print(X_train_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pre-processing the test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 309)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 303)\t0.27862369706954065\n",
      "  (0, 234)\t0.19709709259503683\n",
      "  (0, 219)\t0.2566581404977802\n",
      "  (0, 218)\t0.27862369706954065\n",
      "  (0, 210)\t0.2739599523888023\n",
      "  (0, 197)\t0.21749155805518827\n",
      "  (0, 186)\t0.2739599523888023\n",
      "  (0, 185)\t0.27862369706954065\n",
      "  (0, 182)\t0.2739599523888023\n",
      "  (0, 137)\t0.2566581404977802\n",
      "  (0, 128)\t0.18162009446458494\n",
      "  (0, 124)\t0.27862369706954065\n",
      "  (0, 123)\t0.2411811423673283\n",
      "  (0, 82)\t0.27862369706954065\n",
      "  (0, 80)\t0.27862369706954065\n",
      "  (1, 303)\t0.27862369706954065\n",
      "  (1, 234)\t0.19709709259503683\n",
      "  (1, 219)\t0.2566581404977802\n",
      "  (1, 218)\t0.27862369706954065\n",
      "  (1, 210)\t0.2739599523888023\n",
      "  (1, 197)\t0.21749155805518827\n",
      "  (1, 186)\t0.2739599523888023\n",
      "  (1, 185)\t0.27862369706954065\n",
      "  (1, 182)\t0.2739599523888023\n",
      "  (1, 137)\t0.2566581404977802\n",
      "  :\t:\n",
      "  (23, 219)\t0.12519102664517115\n",
      "  (23, 197)\t0.1060866076048827\n",
      "  (23, 130)\t0.3032390179383435\n",
      "  (23, 128)\t0.08858946005498315\n",
      "  (23, 117)\t0.282626084597179\n",
      "  (23, 112)\t0.3032390179383435\n",
      "  (23, 99)\t0.2535737867797593\n",
      "  (23, 51)\t0.3032390179383435\n",
      "  (23, 36)\t0.24252861576616733\n",
      "  (23, 29)\t0.24252861576616733\n",
      "  (24, 303)\t0.27862369706954065\n",
      "  (24, 234)\t0.19709709259503683\n",
      "  (24, 219)\t0.2566581404977802\n",
      "  (24, 218)\t0.27862369706954065\n",
      "  (24, 210)\t0.2739599523888023\n",
      "  (24, 197)\t0.21749155805518827\n",
      "  (24, 186)\t0.2739599523888023\n",
      "  (24, 185)\t0.27862369706954065\n",
      "  (24, 182)\t0.2739599523888023\n",
      "  (24, 137)\t0.2566581404977802\n",
      "  (24, 128)\t0.18162009446458494\n",
      "  (24, 124)\t0.27862369706954065\n",
      "  (24, 123)\t0.2411811423673283\n",
      "  (24, 82)\t0.27862369706954065\n",
      "  (24, 80)\t0.27862369706954065\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "print(X_test_tfidf.shape)\n",
    "print(type(X_test_tfidf))\n",
    "print(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Testing the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.84\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy : \",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimising the model in two steps\n",
    "### A. Running the Agorithm using a pipeline that includes the following steps\n",
    "   ##### 1. creating Document-term matrix using word frequency of each word in document\n",
    "   ##### 2. creating Document matrix using Tf-idf term weighs\n",
    "   ##### 3. Fitting classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### B. Optimising the algorithm by tunning the Model Parameters using **\"GridSearchCV\"** \n",
    "##### Tunning of Parameters includes-\n",
    "###### 1. we can choose whether to use uni-gram or bi-gram words for vocabulary\n",
    "###### 2. we can choose whether to Enable inverse-document-frequency reweighting or not (Desable by default).\n",
    "###### 3. adjusting the smoothening parameter (alpha  default_value = 1) in naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range' : [(1,1), (1,2)],\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'clf__alpha' : (1e-2, 1e-3)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.001, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting Model using optimal Tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 309)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 109)\t0.35355339059327373\n",
      "  (0, 120)\t0.35355339059327373\n",
      "  (0, 274)\t0.35355339059327373\n",
      "  (0, 86)\t0.35355339059327373\n",
      "  (0, 211)\t0.35355339059327373\n",
      "  (0, 128)\t0.35355339059327373\n",
      "  (0, 239)\t0.35355339059327373\n",
      "  (0, 234)\t0.35355339059327373\n",
      "  (1, 80)\t0.2581988897471611\n",
      "  (1, 124)\t0.2581988897471611\n",
      "  (1, 137)\t0.2581988897471611\n",
      "  (1, 219)\t0.2581988897471611\n",
      "  (1, 186)\t0.2581988897471611\n",
      "  (1, 123)\t0.2581988897471611\n",
      "  (1, 197)\t0.2581988897471611\n",
      "  (1, 182)\t0.2581988897471611\n",
      "  (1, 210)\t0.2581988897471611\n",
      "  (1, 185)\t0.2581988897471611\n",
      "  (1, 303)\t0.2581988897471611\n",
      "  (1, 218)\t0.2581988897471611\n",
      "  (1, 82)\t0.2581988897471611\n",
      "  (1, 128)\t0.2581988897471611\n",
      "  (1, 234)\t0.2581988897471611\n",
      "  (2, 80)\t0.2581988897471611\n",
      "  (2, 124)\t0.2581988897471611\n",
      "  :\t:\n",
      "  (72, 293)\t0.2672612419124244\n",
      "  (72, 127)\t0.2672612419124244\n",
      "  (72, 234)\t0.2672612419124244\n",
      "  (73, 0)\t0.2886751345948129\n",
      "  (73, 230)\t0.2886751345948129\n",
      "  (73, 281)\t0.2886751345948129\n",
      "  (73, 227)\t0.2886751345948129\n",
      "  (73, 208)\t0.2886751345948129\n",
      "  (73, 73)\t0.2886751345948129\n",
      "  (73, 194)\t0.2886751345948129\n",
      "  (73, 212)\t0.2886751345948129\n",
      "  (73, 2)\t0.2886751345948129\n",
      "  (73, 252)\t0.2886751345948129\n",
      "  (73, 147)\t0.2886751345948129\n",
      "  (73, 120)\t0.2886751345948129\n",
      "  (74, 222)\t0.31622776601683794\n",
      "  (74, 176)\t0.31622776601683794\n",
      "  (74, 37)\t0.31622776601683794\n",
      "  (74, 134)\t0.31622776601683794\n",
      "  (74, 36)\t0.31622776601683794\n",
      "  (74, 29)\t0.31622776601683794\n",
      "  (74, 253)\t0.31622776601683794\n",
      "  (74, 197)\t0.31622776601683794\n",
      "  (74, 86)\t0.31622776601683794\n",
      "  (74, 128)\t0.31622776601683794\n"
     ]
    }
   ],
   "source": [
    "vect1 = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "X_train_counts1 = vect1.fit_transform(X_train, y_train)\n",
    "\n",
    "tfidf_transformer1 = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf1 = tfidf_transformer1.fit_transform(X_train_counts1)\n",
    "\n",
    "print(X_train_tfidf1.shape)\n",
    "print(type(X_train_tfidf1))\n",
    "print(X_train_tfidf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MultinomialNB(alpha=0.001)\n",
    "clf1.fit(X_train_tfidf1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 309)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 80)\t0.2581988897471611\n",
      "  (0, 82)\t0.2581988897471611\n",
      "  (0, 123)\t0.2581988897471611\n",
      "  (0, 124)\t0.2581988897471611\n",
      "  (0, 128)\t0.2581988897471611\n",
      "  (0, 137)\t0.2581988897471611\n",
      "  (0, 182)\t0.2581988897471611\n",
      "  (0, 185)\t0.2581988897471611\n",
      "  (0, 186)\t0.2581988897471611\n",
      "  (0, 197)\t0.2581988897471611\n",
      "  (0, 210)\t0.2581988897471611\n",
      "  (0, 218)\t0.2581988897471611\n",
      "  (0, 219)\t0.2581988897471611\n",
      "  (0, 234)\t0.2581988897471611\n",
      "  (0, 303)\t0.2581988897471611\n",
      "  (1, 80)\t0.2581988897471611\n",
      "  (1, 82)\t0.2581988897471611\n",
      "  (1, 123)\t0.2581988897471611\n",
      "  (1, 124)\t0.2581988897471611\n",
      "  (1, 128)\t0.2581988897471611\n",
      "  (1, 137)\t0.2581988897471611\n",
      "  (1, 182)\t0.2581988897471611\n",
      "  (1, 185)\t0.2581988897471611\n",
      "  (1, 186)\t0.2581988897471611\n",
      "  (1, 197)\t0.2581988897471611\n",
      "  :\t:\n",
      "  (23, 128)\t0.25\n",
      "  (23, 130)\t0.25\n",
      "  (23, 197)\t0.25\n",
      "  (23, 219)\t0.25\n",
      "  (23, 234)\t0.25\n",
      "  (23, 247)\t0.25\n",
      "  (23, 249)\t0.25\n",
      "  (23, 253)\t0.25\n",
      "  (23, 258)\t0.25\n",
      "  (23, 272)\t0.25\n",
      "  (24, 80)\t0.2581988897471611\n",
      "  (24, 82)\t0.2581988897471611\n",
      "  (24, 123)\t0.2581988897471611\n",
      "  (24, 124)\t0.2581988897471611\n",
      "  (24, 128)\t0.2581988897471611\n",
      "  (24, 137)\t0.2581988897471611\n",
      "  (24, 182)\t0.2581988897471611\n",
      "  (24, 185)\t0.2581988897471611\n",
      "  (24, 186)\t0.2581988897471611\n",
      "  (24, 197)\t0.2581988897471611\n",
      "  (24, 210)\t0.2581988897471611\n",
      "  (24, 218)\t0.2581988897471611\n",
      "  (24, 219)\t0.2581988897471611\n",
      "  (24, 234)\t0.2581988897471611\n",
      "  (24, 303)\t0.2581988897471611\n"
     ]
    }
   ],
   "source": [
    "X_test_counts1 = vect1.transform(X_test)\n",
    "X_test_tfidf1 = tfidf_transformer1.transform(X_test_counts1)\n",
    "\n",
    "print(X_test_tfidf1.shape)\n",
    "print(type(X_test_tfidf1))\n",
    "print(X_test_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### testing accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.92\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = clf1.predict(X_test_tfidf1)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are not getting an improved accuracy after optimal tuning of parameters may be we should need to collect more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analysing the result Obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  1],\n",
       "       [ 1,  3]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44    @namo_office @narendramodi Yes every Indian ta...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# All false negative\n",
    "print(X_test[y_test > y_pred1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@namo_office @narendramodi Yes every Indian take responsibility all things first election commission take strict actions &amp; strict Rules'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82    @Shehla_Rashid Which congress you mean?\\n1. In...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# All False positive\n",
    "print(X_test[y_test < y_pred1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Shehla_Rashid Which congress you mean?\\n1. Indian national congress \\n2. Congress party of Kerala (Marxist)\\n      Beâ€¦ https://t.co/oKbVF4lflE'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[82]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
